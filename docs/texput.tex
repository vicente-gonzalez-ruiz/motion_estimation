% Emacs, this is -*-latex-*-

\title{\href{https://vicente-gonzalez-ruiz.github.io/motion_estimation}{Motion Estimation}}
% No temporal schemes such as III..., IPP..., IBP..., MCTF ... only basic ME/MC.

\maketitle

\section{Idea}
%{{{

In some 3D signals processed as sequences of 2D frames (for example,
in a video that is a sequence of frames),
\href{https://en.wikipedia.org/wiki/Motion_estimation}{motion
  estimation} techniques find a mapping between such frames. Such
mappings between two or more frames (usually, in the form of one or
more motion vector fields per frame) can be used in motion compensated
transforms, such as Hybrid Coding~\cite{vruiz__hybrid_coding} and
MCTF~\cite{vruiz__MCTF}). Notice that in these examples of temporal
transforms, the motion information must be available also during the
decoding process.

In its simplest form, a motion compensated transform inputs one (or
more) reference frame(s) ${\mathbf R}=\{{\mathbf R}_i\}$, and a motion
vectors field $\overset{{\mathbf R}\rightarrow{\mathbf P}}{\mathbf M}$
that indicates how to project ${\mathbf R}$ onto the predicted frame
${\mathbf P}$, and outputs a prediction frame
\begin{equation}
  \hat{{\mathbf P}} =  \overset{{\mathbf R}\rightarrow {\mathbf P}}{\mathbf M}({\mathbf R}).
  \label{eq:MCP1}
\end{equation}
With this, we compute the residue frame (prediction error)
\begin{equation}
  {\mathbf E} = {\mathbf P} - \hat{\mathbf P}.
\end{equation}

An example of such transformation can be found in the notebook
\href{https://github.com/vicente-gonzalez-ruiz/motion_estimation/blob/main/src/motion_estimation/full_search_block_ME.ipynb}{Full
  search block-based ME}. As it can be seen, the entropy of the motion
compensated redidue has been significantly decreased compared to the
case in which the motion is not compensated.

%{{{ 
\begin{comment}
\begin{figure}
  \centering
  \png{stockholm_R_block}{800}
  \caption{A tile of the first image of the \emph{Stockholm}
    sequence. This is the reference (${\mathbf R}$) frame.}
  \label{fig:R_block}
\end{figure}

\begin{figure}
  \centering
  \png{stockholm_P_block}{800}
  \caption{The same (coordinates) tile of the second image of the
    \emph{stockholm} sequence. This is the predicted (${\mathbf P}$)
    frame.}
  \label{fig:P_block}
\end{figure}

\begin{figure}
  \centering
  \png{stockholm_PR_block}{800}
  \caption{${\mathbf P} - {\mathbf R}$: shows the differences between
    both tiles. The entropy of the residue is displayed between
    parentheses.}
  \label{fig:RP_block}
\end{figure}

\begin{figure}
  \centering
  \png{stockholm_hat_P_block}{800}
  \caption{The prediction frame (${\hat{\mathbf P}}$). See \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/full_search_block_ME.ipynb}{this}.}
  \label{fig:hat_P_block}
\end{figure}

\begin{figure}
  \centering
  \png{stockholm_error_block}{800}
  \caption{The prediction-error frame
    (${\mathbf R} - {\hat{\mathbf P}}$). See
    \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/full_search_block_ME.ipynb}{this}.}
  \label{fig:error_block}
\end{figure}

As it can be seen in the Figures \ref{fig:R_block}, \ref{fig:P_block},
\ref{fig:RP_block}, \ref{fig:hat_P_block}, and \ref{fig:error_block}, the MVs generated
by block-based ME can significantly decrease the entropy.

\begin{figure}
  \centering
  \png{stockholm_MVs_block}{800}
  \caption{Motion vectors to map ${\mathbf P}$ (which is divided into
    disjoint blocks) onto ${\mathbf R}$. See
    \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/full_search_block_ME.ipynb}{this}.}
  \label{fig:MVs_block}
\end{figure}
\end{comment}

%}}}

%}}}

\section{Block-based motion estimation~\cite{rao1996techniques}}
%{{{

\begin{figure}
  \centering
  \svg{graphics/simple}{400}
  \caption{ME using disjoint blocks. $({\mathbf M}_x, {\mathbf M}_y)$
    is the motion vector that indicates where the block $(x,y)$ of
    ${\mathbf P}$ is found in ${\mathbf R}$.}
  \label{fig:simple}
\end{figure}

Block-based ME is the simplest ME algorithm (see the
Fig.~\ref{fig:simple}), ${\mathbf P}$ is divided in blocks of (for
example) 16x16 pixels\footnote{For example, in the MPEG-1 standard,
  the reference image/s is/are divided in blocks of $16\times 16$
  pixels called \emph{macroblocks}.}, and we can use the (R)MSE that
measures the distance in L$_2$ (also known as the Euclidean distance)
between each block of ${\mathbf P}$ and its surrounding pixels in
${\mathbf R}$ (the so called search area)~\cite{zhu2000new}. For each
block, a motion vector that indicates the best match (smaller
distance) is found. The set of motion vectors form the motion vectors
field $\overset{{\mathbf R}\rightarrow {\mathbf P}}{\mathbf M}$ that
obviously, except for a block size of 1x1, will be less dense than
${\mathbf R}$ and ${\mathbf P}$. Notice, however, that, it is not a
good idea to use such a small block size because, in general, the
motion vectors will not describe the true motion in the scene.

However, as it can be seen in the Figure~\ref{fig:MVs_block}, the
motion information computed by the block-based ME algorithm not always
represents the true motion in the scene in the case of using
block-based matching. This can be a drawback, for example, for solving
object tracking problems. In the case of video coding, the main
disadvantage of such issue is that the entropy of the motion fields
increases, which also decreases the compression ratio.

%}}}

\section{Overlapped block matching}

%{{{

\begin{figure}
  \centering
  \svg{graphics/overlaped}{400}
  \caption{ME using overlaped blocks.}
  \label{fig:overlaped}
\end{figure}

A better approximation to the OF for small block sizes can be found if
we allow the blocks to overlap in ${\mathbf
  P}$~\cite{orchard1994overlapped}, case in which the block size for
performing the comparisons must be larger. Again, as it happens with
the disjoint case, only the non overlaped pixels are used for building
the prediction (see the Fig.~\ref{fig:overlaped}). Obviously, the main
drawback of this technique is that it can be more computationally
demanding than the previous one.

\begin{comment}
\begin{figure}
  \centering
  \png{stockholm_hat_P_dense}{800}
  \caption{The prediction frame (${\hat{\mathbf P}}$). See \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/full_search_dense_ME.ipynb}{this}.}
  \label{fig:hat_P_dense}
\end{figure}

\begin{figure}
  \centering
  \png{stockholm_error_dense}{800}
  \caption{The prediction error frame (${\mathbf R} - {\hat{\mathbf P}}$). See \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/full_search_dense_ME.ipynb}{this}.}
  \label{fig:error_dense}
\end{figure}

\begin{figure}
  \centering
  \png{stockholm_MVs_dense}{800}
  \caption{Motion vectors to map ${\mathbf P}$ (from which each pixel has been mapped) onto ${\mathbf R}$. See \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/full_search_dense_ME.ipynb}{this}.}
  \label{fig:MVs_dense}
\end{figure}
\end{comment}

The dense ME algorithm can obtain better predictions than the
block-based one, as it can be seen in the notebook
\href{https://github.com/vicente-gonzalez-ruiz/motion_estimation/blob/main/src/motion_estimation/full_search_dense_ME.ipynb}{Full
  search dense ME}. Notice also that the MVs are also more coherent.

\begin{figure}
  \centering
  \svg{graphics/average}{400}
  \caption{ME using overlaped blocks, averaging the overlaped pixels.}
  \label{fig:average}
\end{figure}

An improvement of the previous technique can also average the
overlaped pixels in the prediction frame $\hat{P}$, as it has been
shown in the Fig.~\ref{fig:average}.

%}}}

\subsection{Machine learning}
%{{{

ANNs (Artifical Neural Networks) can be trained to estimate the motion
between frames~\cite{dosovitskiy2015flownet}. For the training of
ANNs, animation videos are generally used where the motion fields are
known with precision.

%}}}

\section{Subpixel accuracy}
%{{{

Objects in real scenes usually move a rational number of
pixels\footnote{This means that, even if the images are visually
  identical, they have different representation, and therefore,
  ${\mathbf E}\ne{\mathbf 0}$.}, and therefore, the motion information
should provide subpixel displacements.

\begin{figure}
  \svg{graphics/interpolation}{200}
  \caption{Pixel interpolation.}
  \label{fig:interpolation}
\end{figure}

This problem can be mitigated if the predictions are generated after:
(1) interpolate the reference(s) image(s), and (2)
subsample\footnote{This operation implies a filtering to avoid the
  aliasing after the downsampling.} the prediction to the resolution
of the predicted image. For example, in MPEG-1, the motion estimation
can have up to 1/2 pixel accuracy. In this case, a bi-linear
interpolator is used (see the Fig.~\ref{fig:interpolation}).

Unfortunately, the use of subpixel accuracy increases
the entropy of the motion information and, potentially, the number of
motion vectors.

%}}}

\section{Searching strategies}
%{{{

Uauslly, only performed by the compressor.

\begin{figure}
  \svg{graphics/spiral_search}{500}
  \caption{$\pm$ 1 spiral search. Notice that, in the case that all the
    comparisons have the same error, the null motion vector is
    selected. Notice also that the spiral can have any size.}
  \label{fig:spiral_search}
\end{figure}

\subsection{Full (exhaustive) search}
%{{{

All the possibilities are checked (see the
Fig.~\ref{fig:full_search}). Advantage: the highest compression
ratios. Disadvantage: CPU killer. Usually, to maximize the number of
vectors equal to zero, a spiral search is performed (see
Fig.~\cite{fig:spiral_search}).
  
\begin{figure}
  \svg{graphics/full_search}{500}
  \caption{The full search scheme.}
  \label{fig:full_search}
\end{figure}

%}}}

\subsection{Logaritmic search}
%{{{

It is a version of the full search algorithm where the blocks
and the search area are sub-sampled. After finding the best
coincidence, the resolution is increased in a power of 2 and the
previous match is refined in a search area of $\pm 1$, until the
maximal resolution (even using subpixel accuracy) is reached.

%}}}

\subsection{{Telescopic search}}
%{{{ 

Any of the previously described techniques can be accelerated up if
the searching area is reduced. This can be done supposing that the
motion vector of the same block in two consecutive images is similar.

%}}}

\subsection{Optical flow}
%{{{ 

%{{{ 
\begin{comment}
\begin{figure}
  %\begin{tabular}{cccccc}
  %  \png{one} & \png{x} & \png{y} & \png{x2} & \png{y2} & \png{xy} \\
  %\end{tabular}
  \begin{tabular}{cccccc}
    \png{one}{200} & \png{x}{200} & \png{y}{200} & \png{x2}{200} & \png{y2}{200} & \png{xy}{200} \\
    No motion & Constant velocity in $X$ & Constant velocity in $Y$ & Constant acceleration in $X$ & Constant acceleration in $Y$ & Constant accelarion in diagonal
  \end{tabular}
  \caption{Correlation kernels (basis functions) used by the
    \emph{polynomial expansion} of the Farneb{\"a}ck's ME
    algorithm. See \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/farneback_ME.ipynb}{this}. The analized motion is depicted below the plot of each basis.}
  \label{fig:FarnebacK_basis}
\end{figure}
\end{comment}
%}}}

The Optical Flow (OF)~\cite{horn1981determining} describes the aparent
motion of the pixels in the scene between two frames. There are
several OF estimators proposed in the literature, and one of the most
used is the Farneb{\"a}ck's algorithm~\cite{farneback2003two}, which
instead of comparing pixels in the image domain, compares the
coefficients generated by the transform defined by the basis functions
\begin{equation}
    \{1, x, y, x^2, y^2, xy\}
\end{equation}
(see the notebook
\href{https://github.com/vicente-gonzalez-ruiz/motion_estimation/blob/main/src/motion_estimation/farneback_ME.ipynb}{Farnebäck's
  motion estimation}). In this transform domain, the corresponding
subbands quantify the tendency of the image to increase its intensity
in different 2D directions, and therefore, it is more efficient to
know the direction in which the objects are moving.

Farneb{\"a}ck's is a dense OF estimator, which means that we obtain
one motion vector per pixel. This is achieved applying the previous
algorithm to any pixel of the image using a sliding window. It also
provided subpixel accuracy.

%{{{ 

\begin{comment}
\begin{figure}
  \centering
  \png{stockholm_hat_P_farneback}{800}
  \caption{The prediction frame (${\hat{\mathbf P}}$). See \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/farneback_ME.ipynb}{this}.}
  \label{fig:hat_P_farneback}
\end{figure}

\begin{figure}
  \centering
  \png{stockholm_error_farneback}{800}
  \caption{The prediction error frame (${\mathbf R} - {\hat{\mathbf P}}$). See \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/farneback_ME.ipynb}{this}.}
  \label{fig:error_farneback}
\end{figure}

\begin{figure}
  \centering
  \png{stockholm_MVs_farneback}{800}
  \caption{Motion vectors to map ${\mathbf P}$ (from which each pixel has been mapped) onto ${\mathbf R}$. See \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/farneback_ME.ipynb}{this}.}
  \label{fig:MVs_farneback}
\end{figure}
\end{comment}

%}}}

%}}}

\section{ME and RDO}
%{{{

ME can be designed to minimize the distortion $D$ of the residues
after using the MCT (Motion Compensated Transform), or to minimize the
lagrangian
\begin{equation}
  J = R + \lambda D,
\end{equation}
which also takes into consideration the bit-rate $R$. Notice, however,
that in this case the computation of the motion information is also
determined by the bit-rate achieved by the entropy coding of the
motion data and the residues.

Notice that, in general, $D$ will decrease if the ``motion part'' of
$R$ increases. However, if the motion information can be infered by
the decoder, $R$ will be only affected by the entropy encoding of the
residues. On the other hand, when the motion information is infered at
the decoder, this information will be less accurate that if we use all
the visual information avaiable at the encoder.

%}}}

\section{Matching criteria}

%{{{ 

\begin{itemize}
\item
  Let $a$ and $b$ the blocks which we want to compare. Two main
  distortion metrics are commonly used:

  \begin{itemize}
  \item
    \textbf{MSE (Mean Square Error)}: We minimize the energy ${\mathbf E}$ (also known as the L$^2$ distance):

    \begin{equation}
      \frac{1}{16\times 16}\sum_{i=1}^{16}\sum_{j=1}^{16}(a_{ij}-b_{ij})^2
    \end{equation}
  \item
    \textbf{MAE (Mean Absolute Error)}:

    \begin{equation}
      \frac{1}{16\times 16}\sum_{i=1}^{16}\sum_{j=1}^{16}|a_{ij}-b_{ij}|
    \end{equation}
  \end{itemize}
\item
  These similitude measures are used only by MPEG compressors.
  Therefore, any other one with similar effects (such as the error
  variance or the error entropy) could be used also.
\item
  Other less common distortion metrics that can work are:

  \begin{itemize}
  \item
    \textbf{EE (Error
    \href{https://en.wikipedia.org/wiki/Entropy_(information_theory)}{Entropy})}:

    \begin{equation}
      -\frac{1}{16\times 16}\sum_{i=1}^{16}\sum_{j=1}^{16}\log_2(a_{ij}-b_{ij})p(a_{ij}-b_{ij})
    \end{equation}
  \end{itemize}
\end{itemize}

%}}}

\section{References}

\renewcommand{\addcontentsline}[3]{}% Remove functionality of \addcontentsline
\bibliography{motion_estimation,video_compression}

