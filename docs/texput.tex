% Emacs, this is -*-latex-*-

\title{\href{https://vicente-gonzalez-ruiz.github.io/motion_estimation}{Motion Estimation and Compensation in the Image Domain}}
% No temporal schemes such as III..., IPP..., IBP..., MCTF ... only basic ME/MC.

\maketitle

\section{Idea}
%{{{

In some 3D signals processed as sequences of 2D frames (for example,
in a video that is a sequence of frames),
\href{https://en.wikipedia.org/wiki/Motion_estimation}{motion
  estimation} techniques find a mapping between such frames. Such
mappings between two or more frames (usually, in the form of one or
more motion vector fields per frame) can be used in motion compensated
transforms, such as Hybrid Coding~\cite{vruiz__hybrid_coding} and
MCTF~\cite{vruiz__MCTF}). Notice that in these examples of temporal
transforms, the motion information must be available also during the
decoding process.

In its simplest form, a motion compensated transform inputs one (or
more) reference frame(s) ${\mathbf R}=\{{\mathbf R}_i\}$, and a motion
vectors field $\overset{{\mathbf R}\rightarrow{\mathbf P}}{\mathbf M}$
that indicates how to project ${\mathbf R}$ onto the predicted frame
${\mathbf P}$, and outputs a prediction frame
\begin{equation}
  \hat{{\mathbf P}} =  \overset{{\mathbf R}\rightarrow {\mathbf P}}{\mathbf M}({\mathbf R}).
  \label{eq:MCP1}
\end{equation}
With this, we compute the residue frame (prediction error)
\begin{equation}
  {\mathbf E} = {\mathbf P} - \hat{\mathbf P}.
\end{equation}

An example of such transformation can be found in the notebook
\href{https://github.com/vicente-gonzalez-ruiz/motion_estimation/blob/main/src/motion_estimation/full_search_block_ME.ipynb}{Full
  search block-based ME}. As it can be seen, the entropy of the motion
compensated redidue has been significantly decreased compared to the
case in which the motion is not compensated.

%}}}

\section{Subpixel accuracy}
%{{{

Objects in real scenes usually move a rational number of
pixels\footnote{This means that, even if the images are visually
  identical, they have different representation, and therefore,
  ${\mathbf E}\ne{\mathbf 0}$.}, and therefore, the motion information
should provide subpixel displacements.

\begin{figure}
  \svg{graphics/interpolation}{200}
  \caption{Pixel interpolation.}
  \label{fig:interpolation}
\end{figure}

This problem can be mitigated if the predictions are generated after:
(1) interpolate the reference(s) image(s), and (2)
subsample\footnote{This operation implies a filtering to avoid the
  aliasing after the downsampling.} the prediction to the resolution
of the predicted image. For example, in MPEG-1, the motion estimation
can have up to 1/2 pixel accuracy. In this case, a bi-linear
interpolator is used (see the Fig.~\ref{fig:interpolation}).

Unfortunately, the use of subpixel accuracy increases
the entropy of the motion information and, potentially, the number of
motion vectors.

%}}}

\section{Searching strategies}
%{{{

Uauslly, only performed by the compressor.

\begin{figure}
  \svg{graphics/spiral_search}{500}
  \caption{$\pm$ 1 spiral search. Notice that, in the case that all the
    comparisons have the same error, the null motion vector is
    selected. Notice also that the spiral can have any size.}
  \label{fig:spiral_search}
\end{figure}

\subsection{Full (exhaustive) search}
All the possibilities are checked (see the
Fig.~\ref{fig:full_search}). Advantage: the highest compression
ratios. Disadvantage: CPU killer. Usually, to maximize the number of
vectors equal to zero, a spiral search is performed (see
Fig.~\cite{fig:spiral_search}).
  
\begin{figure}
  \svg{graphics/full_search}{500}
  \caption{The full search scheme.}
  \label{fig:full_search}
\end{figure}

\subsection{Logaritmic search}
It is a version of the full search algorithm where the blocks
and the search area are sub-sampled. After finding the best
coincidence, the resolution is increased in a power of 2 and the
previous match is refined in a search area of $\pm 1$, until the
maximal resolution (even using subpixel accuracy) is reached.

\subsection{{Telescopic search}}
Any of the previously described techniques can be accelerated up if
the searching area is reduced. This can be done supposing that the
motion vector of the same block in two consecutive images is similar.

\subsection{Optical flow}

\begin{figure}
  %\begin{tabular}{cccccc}
  %  \png{one} & \png{x} & \png{y} & \png{x2} & \png{y2} & \png{xy} \\
  %\end{tabular}
  \begin{tabular}{cccccc}
    \png{one}{200} & \png{x}{200} & \png{y}{200} & \png{x2}{200} & \png{y2}{200} & \png{xy}{200} \\
    No motion & Constant velocity in $X$ & Constant velocity in $Y$ & Constant acceleration in $X$ & Constant acceleration in $Y$ & Constant accelarion in diagonal
  \end{tabular}
  \caption{Correlation kernels (basis functions) used by the
    \emph{polynomial expansion} of the Farneb{\"a}ck's ME
    algorithm. See \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/farneback_ME.ipynb}{this}. The analized motion is depicted below the plot of each basis.}
  \label{fig:FarnebacK_basis}
\end{figure}

The Optical Flow (OF)describes the aparent motion of the pixels in the
scene. There are several OF estimators proposed in the literature, and
one of the most used is the Farneb{\"a}ck's
algorithm~\cite{farneback2003two}, which instead of comparing pixels
in the image domain, compares the coefficients generated by the
transform defined by the basis functions
\begin{equation}
    \{1, x, y, x^2, y^2, xy\}
\end{equation}
(see the Figure~\ref{fig:FarnebacK_basis}). In this
transform\footnote{Which is applied by overlapped regions.} domain,
the corresponding subbands quantify the tendency of the image to
increase its intensity in different 2D directions, and therefore, it
is more efficient to know the direction in which the objects are
moving.

\begin{figure}
  \centering
  \png{stockholm_hat_P_farneback}{800}
  \caption{The prediction frame (${\hat{\mathbf P}}$). See \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/farneback_ME.ipynb}{this}.}
  \label{fig:hat_P_farneback}
\end{figure}

\begin{figure}
  \centering
  \png{stockholm_error_farneback}{800}
  \caption{The prediction error frame (${\mathbf R} - {\hat{\mathbf P}}$). See \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/farneback_ME.ipynb}{this}.}
  \label{fig:error_farneback}
\end{figure}

\begin{figure}
  \centering
  \png{stockholm_MVs_farneback}{800}
  \caption{Motion vectors to map ${\mathbf P}$ (from which each pixel has been mapped) onto ${\mathbf R}$. See \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/farneback_ME.ipynb}{this}.}
  \label{fig:MVs_farneback}
\end{figure}

The Farneback's ME is a dense ME, and it provides subpixel
interpolation because the MVs are real numbers (see the
Figures~\ref{fig:hat_P_farneback}, \ref{fig:error_farneback} and
\ref{fig:MVs_farneback}). Notice that the prediction is the best of
the all tested algorithms, probably by the subpixel accuracy.

%%%%%%%%%%%%%


\section{RDO}
%{{{

% Posiblemente solapa con el el siguiente párrafo
It is obvious that if $\hat{{\mathbf P}}={\mathbf P}$ the redidues
${\mathbf P}$ will be zero, i.e., the RD tradeoff will be
good. However, we must consider also the bit-rate of the encoding of
$\overset{{\mathbf R}\rightarrow {\mathbf P}}{\mathbf M}$. Therefore,
in order to optimize the RD curve, we must consider at the same time
the entropy of the residues ${\mathbf E}$ and the entropy of the
motion information $\overset{{\mathbf R}\rightarrow {\mathbf
    P}}{\mathbf M}$.

% Posiblemente solapado con el anterior párrafo
ME can be designed to minimize the distortion $D$ of the residues
after using the MCT (Motion Compensated Transform) of to minimize the
lagrangian
\begin{equation}
  J = R + \lambda D,
\end{equation}
which also takes into consideration the bit-rate $R$. Notice, however,
that in this case the computation of the motion information is also
determined by the bit-rate achieved by the entropy coding of the
motion data and the residues.

Notice that, in general, $D$ will decrease if the ``motion part'' of
$R$ increases. However, if the motion information can be infered by
the decoder, $R$ will be only affected by the entropy encoding of the
residues. On the other hand, when the motion information is infered at
the decoder, this information will be less accurate that if we use all
the visual information avaiable at the encoder.

%}}}

\section{Perceptual issues}
%{{{

\begin{figure}
  \centering
  \png{stockholm_MVs_block}{800}
  \caption{Motion vectors to map ${\mathbf P}$ (which is divided into
    disjoint blocks) onto ${\mathbf R}$. See
    \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/full_search_block_ME.ipynb}{this}.}
  \label{fig:MVs_block}
\end{figure}

As it can be seen in the Figure~\ref{fig:MVs_block}, the motion
information computed by a ME algorithm not always represents the true
motion in the scene in the case of using block-based matching. In
non-reliable transmission environments this issue also dificults the
prediction of the missing motion information. Notice also that the
entropy of the motion data increases.

%}}}


\section{Types of macro-blocks -> RDO}

The MC schemes used in most video coding standards compensate blocks
of pixels. In this context, depending of the block
decision mode implemented in the RDO procedure\footnote{Obviously, the
  part of the RDO procedure that controls the block-type.}, blocks can
be of different type~\cite{vruiz__MEC}
\begin{enumerate}
\item \textbf{I-type (intra)}, when the block is encoded without
  considering any other external reference information. In this case,
  no resudue is generated), and a I-type block represents only
  texture. RDO selects this mode when the compression of residue block
  generates more bits than the original (predicted) one.
\item \textbf{P-type (predicted)}, if there is one reference (block)
  for the (predicted) block, which belongs to a previous frame. Now we
  encode a block with residual texture and a motion vector per
  block. P-type blocks require less data than I-type blocks (this is
  guaranteed by the RDO).
\item \textbf{B-type (bidirectionally-predicted)}, if there are two or
  more references for the block, anterior (past) and posterior
  (future). By definition (RDO), it is more RD-advantageous to use
  B-type blocks compared to P-type blocks, because the entropy of the
  residual texture decreases (even considering that now we need one
  motion vector per reference).
\item \textbf{S-type (skipped)}, if the residue is so small that it is
  more beneficial to consider that it is zero. Consequently, the block
  is a copy of the reference.
\end{enumerate}

\begin{figure}
  \centering
  \myfig{graphics/macroblocks}{6cm}{600}
  \caption{Types of macro-blocks.}
  \label{fig:macroblocks}
\end{figure}

Ideally, the type (also called, mode) of the macro-blocks is decided
considering all the posibilities (I, P, B, or S) and selecting the
most beneficial one from a RD perpective, i.e., selecting the
alternative that minimizes the RD cost. This RDO is performed at the
macro-block level.

The decision of the type of each macro-block is performed at the
compressor, and this information is signaled in the code-stream. An
visual example of the decision of the type of the macro-blocks is
shown in the Figure~\ref{fig:macroblocks}.

\section{Matching criteria}
\begin{itemize}
\item
  Let $a$ and $b$ the macroblocks which we want to compare. Two main
  distortion metrics are commonly used:

  \begin{itemize}
  \item
    \textbf{MSE (Mean Square Error)}:

    \begin{equation}
      \frac{1}{16\times 16}\sum_{i=1}^{16}\sum_{j=1}^{16}(a_{ij}-b_{ij})^2
    \end{equation}
  \item
    \textbf{MAE (Mean Absolute Error)}:

    \begin{equation}
      \frac{1}{16\times 16}\sum_{i=1}^{16}\sum_{j=1}^{16}|a_{ij}-b_{ij}|
    \end{equation}
  \end{itemize}
\item
  These similitude measures are used only by MPEG compressors.
  Therefore, any other one with similar effects (such as the error
  variance or the error entropy) could be used also.
\item
  Other less common distortion metrics that can work are:

  \begin{itemize}
  \item
    \textbf{EE (Error
    \href{https://en.wikipedia.org/wiki/Entropy_(information_theory)}{Entropy})}:

    \begin{equation}
      -\frac{1}{16\times 16}\sum_{i=1}^{16}\sum_{j=1}^{16}\log_2(a_{ij}-b_{ij})p(a_{ij}-b_{ij})
    \end{equation}
  \end{itemize}
\end{itemize}

\section{Linear frame interpolation using block-based motion compensation}
\label{sec:linear_frame_interpolation}
\begin{figure}[h]
  \svg{graphics/frame_interpolation}{900}
  \caption{Frame interpolation.}
  \label{fig:frame_interpolation}
\end{figure}

\subsection*{Input}
\begin{itemize}
\tightlist
\item
  $R$: square search area, in pixels.
\item
  $B$: square block size, in pixels.
\item
  $O$: border size, in pixels.
\item
  $s_i$, $s_j$ and $s_k$ three chronologically ordered,
  equidistant frames, with resolution $X\times Y$.
\item
  $A$: $\frac{1}{2^A}$ subpixel accuracy.
\end{itemize}

\subsection*{Output}
\begin{itemize}
\tightlist
\item
  $\hat{s}_j$: a prediction for frame $s_j$.
\item
  $m$: a matrix with $\lceil X/B\rceil \times \lceil Y/B\rceil$
  bidirectional motion vectors.
\item
  $e$: a matrix with $\lceil X/B\rceil \times \lceil Y/B\rceil$
  bidirectional Root Mean Square matching Wrrors (RMSE).
\end{itemize}

\subsection*{Algorithm}
\begin{enumerate}
\tightlist

\item
  Compute the DWT$^l$, where $l=\lfloor\log_2(R)\rfloor-1$ levels,
  of the predicted frame $s_j$ and the two reference frames $s_i$
  and $s_k$.
  \href{https://vicente-gonzalez-ruiz.github.io/video_compression/graphics/frame_interpolation_step_1.svg}{Example}.

\item
  $LL^l(m)\leftarrow 0$, or any other precomputed values (for example,
  from a previous ME in neighbor frames).
  \href{https://vicente-gonzalez-ruiz.github.io/video_compression/graphics/frame_interpolation_step_2.svg}{Example}.

\item
  Divide the subband $LL^l(s_j)$ into blocks of size $B\times B$
  pixels, and $\pm 1$-spiral-search them in the subbands $LL^l(s_i)$
  and $LL^l(s_k)$, calculating a low-resolution
  $LL^l(m)=\{LL^l(\overleftarrow{m}), LL^l(\overrightarrow{m})\}$
  bi-directional motion vector field. 
  \href{https://vicente-gonzalez-ruiz.github.io/video_compression/graphics/frame_interpolation_step_3A.svg}{Example}.
  \href{https://vicente-gonzalez-ruiz.github.io/video_compression/graphics/frame_interpolation_step_3A_bis.svg}{Example}.
\item
  While $l>0$:
  
  \begin{enumerate}
    
  \item
    Synthesize $LL^{l-1}(m)$, $LL^{l-1}(s_j)$, $LL^{l-1}(s_i)$
    and $LL^{l-1}(s_k)$, by computing the 1-level DWT$^{-1}$.
    \href{https://vicente-gonzalez-ruiz.github.io/video_compression/graphics/frame_interpolation_step_4A.svg}{Example}.
    \href{https://vicente-gonzalez-ruiz.github.io/video_compression/graphics/frame_interpolation_step_4A_bis.svg}{Example}

  \item
    $LL^{l-1}(M)\leftarrow LL^{l-1}(M)\times 2$.
    \href{https://vicente-gonzalez-ruiz.github.io/video_compression/graphics/frame_interpolation_step_4B.svg}{Example}.
  
  \item
    Refine $LL^{l-1}(m)$ using $\pm 1$-spiral-search.
    \href{https://vicente-gonzalez-ruiz.github.io/video_compression/graphics/frame_interpolation_step_4C.svg}{Example}.
  
  \item
  $l\leftarrow l-1$. (When $l=0$, the motion vectors field $m$
  has the structure:)
  
  \end{enumerate}
  
  \href{https://vicente-gonzalez-ruiz.github.io/video_compression/graphics/motion_vectors.svg}{Example}.
  
\item
  While $l<A$ (in the first iteration, $l=0$, and $LL^0(M):=M$):
  
  \begin{enumerate}
  \item
    $l\leftarrow l+1$.
    
  \item
    Synthesize $LL^{-l}(s_j)$, $LL^{-l}(s_i)$ and $LL^{-l}(s_k)$,
    computing the 1-level DWT$^{-1}$ (high-frequency subbands are
    $0$). This performs a zoom-in in these frames using $1/2$-subpixel
    accuracy. 
    
    \href{https://vicente-gonzalez-ruiz.github.io/video_compression/graphics/frame_interpolation_step_5B.svg}{Example}.
    
  \item
    $m\leftarrow m\times 2$.
    
    \href{https://vicente-gonzalez-ruiz.github.io/video_compression/graphics/motion_vectors_by_2.svg}{Example}.
    
  \item
    $B\leftarrow B\times 2$.
    
  \item
    Divide the subband $LL^{-l}(s_j)$ into blocks of $B\times B$
    pixels and $\pm 1$-spiral-search them into the subbands
    $LL^{-l}(s_i)$ and $LL^{-l}(s_k)$, calculating a $1/2^l$
    sub-pixel accuracy $m$ bi-directional motion vector field. 
    \href{https://vicente-gonzalez-ruiz.github.io/video_compression/graphics/motion_vectors_definitive.svg}{Example}.
    
  \item
    Frame prediction. For each block $b$:
    
  \item
    Compute
    \begin{equation}
      \hat{b}\leftarrow \frac{b_i\big(\overleftarrow{e}_{\text{max}}-\overleftarrow{e}(b)\big) + b_k\big(\overrightarrow{e}_{\text{max}}-\overrightarrow{e}(b)\big)}{\big(\overleftarrow{e}_{\text{max}}-\overleftarrow{e}(b)\big) + \big(\overrightarrow{e}_{\text{max}}-\overrightarrow{e}(b)\big)},
    \end{equation}
    
    where $\overleftarrow{e}(b)$ is the (minimum) distortion of the
    best backward matching for block $b$, $\overrightarrow{e}(b)$
    the (minimum) distortion of the best forward matching for block
    $b$,
    $\overleftarrow{e}_{\text{max}}=\overrightarrow{e}_{\text{max}}$ are
    the backward and forward maximum matching distortions, $b_i$ is
    the (backward) block found (as the most similar to $b$) in frame
    $s_i$ and $b_k$ is the (forward) block found in frame
    $s_k$. Notice that, if
    $\overleftarrow{e}(b)=\overrightarrow{e}(b)$, then the
    prediction is
    \begin{equation}
      \hat{b} = \frac{b_i + b_k}{2},
    \end{equation}
    and if $\overleftarrow{e}(b)=0$,
    \begin{equation}
      \hat{b} = b_k,
    \end{equation} and viceversa.
  \end{enumerate}
\end{enumerate}


\section{References}
%{{{

\renewcommand{\addcontentsline}[3]{}% Remove functionality of \addcontentsline
\bibliography{image_pyramids,DWT,motion_estimation,HEVC}

%}}}


%%%%%%%%%%%

\subsection{ME for what?}

Specifically, a MCP (MC Predictor) inputs one (or more) reference
frame(s) ${\mathbf R}=\{{\mathbf R}_i\}$, and a motion vectors field
$\overset{{\mathbf R}\rightarrow{\mathbf P}}{\mathbf M}$ that
indicates how to project ${\mathbf R}$ onto the predicted frame ${\mathbf P}$, and outputs
a prediction frame
\begin{equation}
  \hat{{\mathbf P}} =  \overset{{\mathbf R}\rightarrow {\mathbf P}}{\mathbf M}({\mathbf R}).
  \label{eq:MCP1}
\end{equation}

%(probably, the closest one to the
%predicted frame), and therefore, we don't need to specify which are
%the reference images to make the prediction. For this reason,
%Eq,~\ref{eq:MCP1} can be rewritten as
%\begin{equation}
%  \hat{\mathbf W}_k = \overset{{\mathbf W}_{k-1}\rightarrow {\mathbf
%  W}_{k}}{\mathbf M}({\mathbf W}_{k-1}) = \overset{(k-1)\rightarrow
%          k}{\mathbf M}({\mathbf W}_{k-1})
%%  \hat{{\mathbf P}_k} = \overset{{\mathbf P}_k\rightarrow {\mathbf
%%  P}_{k-1}}{\mathbf M}({\mathbf P}_{k-1}) = \overset{k\rightarrow
%%          k-1}{\mathbf M},
%  \label{eq:MCP2}
%\end{equation}

\subsection{But ... what exactly do we need?}
Our main objective is to minimize the differences (for example, the
\href{https://en.wikipedia.org/wiki/Euclidean_distance}{L$_2$
  distance}) between ${\mathbf P}$ (the predicted frame) and $\hat{\mathbf P}$ (the
prediction frame), i.e. minimizing
\begin{equation}
  {\mathbf E} = {\mathbf P} - \hat{\mathbf P}
\end{equation}
in order to get that ${\mathbf E}$ will be more compressible than
${\mathbf P}$. To achieve this, we can compute $\overset{{\mathbf
    R}\rightarrow {\mathbf P}}{\mathbf M}$ that simply minimizes the
L$_2$ energy of ${\mathbf E}$, $||{\mathbf E}||^2$, or we can compute
a $\overset{{\mathbf R}\rightarrow {\mathbf P}}{\mathbf M}$ that also describes the Optical
Flow~\cite{horn1981determining} (OF) between the pixels of ${\mathbf
  R}$ and ${\mathbf P}$, that although not necessarily has to
minimize $||{\mathbf E}||^2$, tries to show the true movement of the
pixels between the both frames. This second option has the advantage
of generating more visually pleasing reconstructions when the
code-stream is partially received and makes easier to predict the
content of the motion fields.

The first type of techniques are simply called ``ME techniques'', and
are usually faster\footnote{Obviously, depending on the algorithm.}
than the second type, based on the estimation of the OF.

%Let's see two basic techniques to estimate the motion between 2
%frames, $R$ and $P$. In this discussion it will be supposed that the
%motion of the objects that are in both frames is bounded, and that the
%luminance varies smoothly between adjacent frames.

Now, let's see some of the most used techniques for estimating the
motion between two frames. Notice that, in general, better estimations
can be found if we suppose motion models such as that the objects
exhibit
\href{https://en.wikipedia.org/wiki/Inertia}{inertia}. However, this
case will not be considered for now.

\subsection{Block-based motion estimation~\cite{rao1996techniques}}

\begin{figure}
  \centering
  \svg{graphics/simple}{400}
  \caption{ME using disjoint blocks. $({\mathbf M}_x, {\mathbf M}_y)$
    is the motion vector that indicates where the block $(x,y)$ of
    ${\mathbf P}$ is found in ${\mathbf R}$.}
  \label{fig:simple}
\end{figure}

Block-based ME is the simplest ME algorithm (see the
Fig.~\ref{fig:simple}), ${\mathbf P}$ is divided in blocks of (for
example) 16x16 pixels\footnote{For example, in the MPEG-1 standard,
  the reference image/s is/are divided in blocks of $16\times 16$
  pixels called \emph{macroblocks}.}, and we can use the (R)MSE that
measures the distance in L$_2$ (also known as the Euclidean distance)
between each block of ${\mathbf P}$ and its surrounding pixels in
${\mathbf R}$ (the so called search area)~\cite{zhu2000new}. For each
block, a motion vector that indicates the best match (smaller
distance) is found. The set of motion vectors form the motion vectors
field $\overset{{\mathbf R}\rightarrow {\mathbf P}}{\mathbf M}$ that
obviously, except for a block size of 1x1, will be less dense than
${\mathbf R}$ and ${\mathbf P}$. Notice, however, that, it is not a
good idea to use such a small block size because, in general, the
motion vectors will not describe the true motion in the scene.

\begin{figure}
  \centering
  \png{stockholm_R_block}{800}
  \caption{A tile of the first image of the \emph{Stockholm}
    sequence. This is the reference (${\mathbf R}$) frame.}
  \label{fig:R_block}
\end{figure}

\begin{figure}
  \centering
  \png{stockholm_P_block}{800}
  \caption{The same (coordinates) tile of the second image of the
    \emph{stockholm} sequence. This is the predicted (${\mathbf P}$)
    frame.}
  \label{fig:P_block}
\end{figure}

\begin{figure}
  \centering
  \png{stockholm_PR_block}{800}
  \caption{${\mathbf P} - {\mathbf R}$: shows the differences between
    both tiles. The entropy of the residue is displayed between
    parentheses.}
  \label{fig:RP_block}
\end{figure}

\begin{figure}
  \centering
  \png{stockholm_hat_P_block}{800}
  \caption{The prediction frame (${\hat{\mathbf P}}$). See \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/full_search_block_ME.ipynb}{this}.}
  \label{fig:hat_P_block}
\end{figure}

\begin{figure}
  \centering
  \png{stockholm_error_block}{800}
  \caption{The prediction-error frame
    (${\mathbf R} - {\hat{\mathbf P}}$). See
    \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/full_search_block_ME.ipynb}{this}.}
  \label{fig:error_block}
\end{figure}

As it can be seen in the Figures \ref{fig:R_block}, \ref{fig:P_block},
\ref{fig:RP_block}, \ref{fig:hat_P_block}, and \ref{fig:error_block}, the MVs generated
by block-based ME can significantly decrease the entropy.

\begin{figure}
  \centering
  \png{stockholm_MVs_block}{800}
  \caption{Motion vectors to map ${\mathbf P}$ (which is divided into
    disjoint blocks) onto ${\mathbf R}$. See
    \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/full_search_block_ME.ipynb}{this}.}
  \label{fig:MVs_block}
\end{figure}

However, as it can be seen in the Figure~\ref{fig:MVs_block}, the
motion information computed by the block-based ME algorithm not always
represents the true motion in the scene in the case of using
block-based matching. This can be a drawback, for example, for solving
object tracking problems. In the case of video coding, the main
disadvantage of such issue is that the entropy of the motion fields
increases, which also decreases the compression ratio.

\subsection{Overlapped block matching}

\begin{figure}
  \centering
  \svg{graphics/overlaped}{400}
  \caption{ME using overlaped blocks.}
  \label{fig:overlaped}
\end{figure}

A better approximation to the OF for small block sizes can be found if
we allow the blocks to overlap in ${\mathbf
  P}$~\cite{orchard1994overlapped}, case in which the block size for
performing the comparisons must be larger. Again, as it happens with
the disjoint case, only the non overlaped pixels are used for building
the prediction (see the Fig.~\ref{fig:overlaped}). Obviously, the main
drawback of this technique is that it can be more computationally
demanding than the previous one.

\begin{figure}
  \centering
  \png{stockholm_hat_P_dense}{800}
  \caption{The prediction frame (${\hat{\mathbf P}}$). See \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/full_search_dense_ME.ipynb}{this}.}
  \label{fig:hat_P_dense}
\end{figure}

\begin{figure}
  \centering
  \png{stockholm_error_dense}{800}
  \caption{The prediction error frame (${\mathbf R} - {\hat{\mathbf P}}$). See \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/full_search_dense_ME.ipynb}{this}.}
  \label{fig:error_dense}
\end{figure}

\begin{figure}
  \centering
  \png{stockholm_MVs_dense}{800}
  \caption{Motion vectors to map ${\mathbf P}$ (from which each pixel has been mapped) onto ${\mathbf R}$. See \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/full_search_dense_ME.ipynb}{this}.}
  \label{fig:MVs_dense}
\end{figure}

The dense ME algorithm can obtain better predictions than the
block-based one, as it can be seen in the
Figures~\ref{fig:hat_P_dense} and \ref{fig:error_dense}. The MVs are
also more coherent (see Figure~\ref{fig:MVs_dense}).

\begin{figure}
  \centering
  \svg{graphics/average}{400}
  \caption{ME using overlaped blocks, averaging the overlaped pixels.}
  \label{fig:average}
\end{figure}

An improvement of the previous technique can also average the
overlaped pixels in the prediction frame $\hat{P}$, as it has been
shown in the Fig.~\ref{fig:average}.

\subsection{Machine learning}
ANNs (Artifical Neural Networks) can be trained to estimate the motion
between frames~\cite{dosovitskiy2015flownet}. For the training of
ANNs, animation videos are generally used where the motion fields are
known with precision.



\section{What do I have to do?}

\begin{figure}
  \centering
  \myfig{graphics/problem}{3cm}{300}
  \caption{Basic encoding problem.}
  \label{fig:problem}
\end{figure}

Using the encoding system described in the Figure~\ref{fig:problem}, and defined by
\begin{equation}
  \left\{\
    \begin{array}{l}
      \tilde{\mathbf R} = \text{Q}_{\mathbf R}({\mathbf R}) \\
      \tilde{\mathbf E} = \text{Q}_{\mathbf E}\big({\mathbf P}-\overset{{\mathbf R}\rightarrow {\mathbf P}}{\mathbf M}(\tilde{\mathbf R})\big)
    \end{array}
  \right.
  \label{eq:forward}
\end{equation}
and
\begin{equation}
  \begin{array}{l}
    \tilde{\mathbf P} = \tilde{\mathbf E} + \overset{{\mathbf R}\rightarrow {\mathbf P}}{\mathbf M}(\tilde{\mathbf R}),
  \end{array}
  \label{eq:backward}
\end{equation}

find $\text{Q}_{\mathbf{R}}$ and $\text{Q}_{\mathbf{E}}$ that minimize in the RD domain (the RD curve of)
\begin{equation}
  \text{MSE}(\{\mathbf{R},\mathbf{P}\},\{\hat{\mathbf{R}},\hat{\mathbf{P}}\}) = \frac{\text{MSE}({\mathbf R},\hat{\mathbf R}) + \text{MSE}({\mathbf P},\hat{\mathbf P})}{2},
\end{equation}
set that
\begin{equation}
  \text{MSE}({\mathbf R},\tilde{\mathbf R}) = \text{MSE}({\mathbf P},\tilde{\mathbf P}).
  \label{eq:constant_quality}
\end{equation}
Equation~\ref{eq:constant_quality} indicates that all the decoded
frames should have the same distortion (from a human perception point
of view). Notice that the transform defined by the Equations
~\ref{eq:forward} and \ref{eq:backward} is not orthogonal and
therefore, the ``subbands'' $\tilde{\mathbf R}$ and
$\tilde{\mathbf P}$ are not independent. It can be seen that
$\text{Q}_{\mathbf R}$ affects to the selection of
$\text{Q}_{\mathbf E}$, because $\tilde{\mathbf R}$ is used as
reference for finding ${\mathbf E}$.


\section{References}

\renewcommand{\addcontentsline}[3]{}% Remove functionality of \addcontentsline
\bibliography{image_pyramids,DWT,motion_estimation,HEVC}

